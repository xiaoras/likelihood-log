[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Likelihood Log",
    "section": "",
    "text": "Quarto Basics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion\n\n\n\nquant\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nAndrea Dapor\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 29, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/BESS-revenues.html",
    "href": "posts/BESS-revenues.html",
    "title": "1. Get Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nfrom tshistory.api import timeseries\nimport holidays\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\n\nimport shap\n\nfrom scipy.optimize import nnls\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nFirst, we define two series:\nThen, we use the following prices for Ancillary Services:\nNote that the “secondary reserve” is the price we get paid for the simple fact we give our availability to bajar/subir; the actual energy dispatch is remunerated at the “secondary energy” price.\nConcerning DA price and Intraday Continuous Market (ICM), we take the following:\nconfig_data_connector = {\n    'iberia': 'https://saturn-iberia.gem.myengie.com/api',\n    'power': 'https://saturn-power.gem.myengie.com/api',\n    'hydro': 'https://saturn-hydro.gem.myengie.com/api',\n    'gas': 'https://saturn-gas.gem.myengie.com/api',\n}\n\nconfig_iberia = {\n    # utilities\n    'price_mfrr_scheduled': 'ib.power.sp.price.mfrr_scheduled.eur_mwh.15m.obs.esios',\n    'energy_tertiary_down': 'ib.power.sp.energy.tertiary_down.mwh.15m.obs.esios', # 10394\n    'energy_tertiary_up': 'ib.power.sp.energy.tertiary_up.mwh.15m.obs.esios', # 10395\n    \n    # OMIE\n    'DA_price': 'ib.power.sp.price.eur_mwh.h.obs.dm.omie',\n    # 'subastas_price': 'ib.power.sp.price.icm.eur_mwh.h.obs.mi3.omie', # ???\n    'ICM_price': 'ib.power.sp.price.icm.eur_mwh.h.obs.id3.omie',\n    \n    # RR\n    'rr_down_price': 'ib.power.sp.price.rr_down.eur_mwh.h.obs.esios', # 10385 (weighted average price downward satisfied needs of replacement reserve)\n    'rr_up_price': 'ib.power.sp.price.rr_up.eur_mwh.h.obs.esios', # 10384 (weighted average price upward satisfied needs of replacement reserve)\n    # 'rr_down_mwh': 'ib.power.sp.energy.rr_down.nec.mwh.h.obs.esios',\n    # 'rr_up_mwh': 'ib.power.sp.energy.rr_up.nec.mwh.h.obs.esios',\n    \n    # TERTIARY\n    'tertiary_down_price_old': 'ib.power.sp.price.tertiary_down.eur_mwh.15m.obs.esios', # 676 (old downward scheduled activation tertiary reserve price)\n    'tertiary_up_price_old': 'ib.power.sp.price.tertiary_up.eur_mwh.15m.obs.esios', # 677 (old upward scheduled activation tertiary reserve price)\n    # 'tertiary_down_mwh': 'ib.power.sp.energy.tertiary_down.mwh.15m.obs.esios',\n    # 'tertiary_up_mwh': 'ib.power.sp.energy.tertiary_up.mwh.15m.obs.esios',\n    \n    # SECUNDARY\n    'secondary_down_price': 'ib.power.sp.price.secondary_down.eur_mwh.15m.obs.esios', # 683 (aFRR energy marginal price downward)\n    'secondary_up_price': 'ib.power.sp.price.secondary_up.eur_mwh.15m.obs.esios', # 682 (aFRR energy marginal price upward)\n    'secondary_capacity_price_old': 'ib.power.sp.price.secondaryband.eur_mwh.15m.obs.esios', # 634 (old aFRR reserve marginal single price)\n    'secondary_capacity_down_price_new': 'ib.power.sp.price.secondaryreserve_down.eur_mw.15m.obs.esios',\n    'secondary_capacity_up_price_new': 'ib.power.sp.price.secondaryreserve_up.eur_mw.15m.obs.esios',\n    # 'secondary_net_energy_mwh': 'ib.power.sp.energy.secondary_net_energy.mwh.h.obs.esios',\n    # 'secondary_energy_up_mwh': 'ib.power.sp.energy.secondary_up.mwh.h.obs.esios',\n    # 'secondary_energy_down_mwh': 'ib.power.sp.energy.secondary_down.mwh.h.obs.esios',\n    # 'secondaryband_down_mw': 'ib.power.sp.energy.secondaryband_down.mw.15m.obs.esios',\n    # 'secondaryband_up_mw': 'ib.power.sp.energy.secondaryband_up.mw.15m.obs.esios',\n}\n\nconfig_power = {\n}\n\nconfig_hydro = {\n}\n\nconfig_gas = {\n}\n\n# build config dict\n\nconfig_dict = {\n    'iberia': config_iberia,\n    'power': config_power,\n    'hydro': config_hydro,\n    'gas': config_gas,\n}\nstart_date = pd.to_datetime('2023-01-01').tz_localize('UTC')\nend_date = pd.to_datetime('2025-12-31').tz_localize('UTC')\n\nseries_list = []\nfor saturn_instance_name, saturn_instance_url in config_data_connector.items():\n    ts_api = timeseries(saturn_instance_url)\n    ts_api.session.verify = False\n    for ts_name, ts_id in config_dict[saturn_instance_name].items():\n        series = ts_api.get(ts_id, from_value_date=start_date)\n        series.name = ts_name\n        if series.index.tz is None:\n            series.index = series.index.tz_localize('UTC')\n        series_list.append(series)\n\ndata = pd.concat(series_list, axis=1)\ndata = data.fillna(0)\ndata = data[:end_date]\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'saturn-iberia.gem.myengie.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\nBuild all series we need.\nprice_mfrr_single = data['price_mfrr_scheduled']\n\ndata['mfrr_scheduled_down'] = np.where(data['energy_tertiary_up'] &lt;= data['energy_tertiary_down'], price_mfrr_single, 0)\ndata['mfrr_scheduled_up'] = np.where(data['energy_tertiary_up'] &gt; data['energy_tertiary_down'], price_mfrr_single, 0)\ncutoff = pd.Timestamp(\"2024-12-10 08:45\", tz=\"UTC\")\ndata['tertiary_down_price'] = data['tertiary_down_price_old'].where(data.index &lt;= cutoff, data['mfrr_scheduled_down'])\ndata['tertiary_up_price'] = data['tertiary_up_price_old'].where(data.index &lt;= cutoff, data['mfrr_scheduled_up'])\ncutoff = pd.Timestamp(\"2024-11-19 22:45\", tz=\"UTC\")\ndata['secondary_capacity_down_price'] = data['secondary_capacity_price_old'].where(data.index &lt;= cutoff, data['secondary_capacity_down_price_new'])\ndata['secondary_capacity_up_price'] = data['secondary_capacity_price_old'].where(data.index &lt;= cutoff, data['secondary_capacity_up_price_new'])\nKeep only the relevant series.\nprices = data[[\n    'DA_price',\n    'ICM_price',\n    'rr_down_price',\n    'rr_up_price',\n    'tertiary_down_price',\n    'tertiary_up_price',\n    'secondary_down_price',\n    'secondary_up_price',\n    'secondary_capacity_down_price',\n    'secondary_capacity_up_price',\n]]\n\n# remove crazy values\n\nprices = prices.mask(abs(prices) &gt; 5000, np.nan)"
  },
  {
    "objectID": "posts/BESS-revenues.html#daily-correlation-among-markets",
    "href": "posts/BESS-revenues.html#daily-correlation-among-markets",
    "title": "1. Get Data",
    "section": "3.1 Daily Correlation among Markets",
    "text": "3.1 Daily Correlation among Markets\n\nprices_daily = prices_clean.groupby(prices_clean.index.date).transform(\"mean\")\nprices_norm = prices_clean - prices_daily\n\n\nprices_norm.corr()\n\n\n\n\n\n\n\n\nDA_price\nICM_price\nrr_price\ntertiary_price\nrr_down_price\nrr_up_price\ntertiary_down_price\ntertiary_up_price\n\n\n\n\nDA_price\n1.000000\n0.945256\n0.076493\n0.012227\n0.218516\n0.417982\n0.019240\n0.030822\n\n\nICM_price\n0.945256\n1.000000\n0.092414\n0.015239\n0.209366\n0.421165\n0.015739\n0.029887\n\n\nrr_price\n0.076493\n0.092414\n1.000000\n0.079225\n-0.432422\n0.674087\n-0.048634\n0.070120\n\n\ntertiary_price\n0.012227\n0.015239\n0.079225\n1.000000\n-0.028462\n0.066471\n-0.578692\n0.922843\n\n\nrr_down_price\n0.218516\n0.209366\n-0.432422\n-0.028462\n1.000000\n0.073876\n0.040829\n-0.011254\n\n\nrr_up_price\n0.417982\n0.421165\n0.674087\n0.066471\n0.073876\n1.000000\n-0.027502\n0.068049\n\n\ntertiary_down_price\n0.019240\n0.015739\n-0.048634\n-0.578692\n0.040829\n-0.027502\n1.000000\n-0.343035\n\n\ntertiary_up_price\n0.030822\n0.029887\n0.070120\n0.922843\n-0.011254\n0.068049\n-0.343035\n1.000000\n\n\n\n\n\n\n\nLet’s compute the correlation daily, so as to have a daily time series for each pair of prices.\nFor simplicity, we do it only between any market and DA.\n\nfeature1 = 'DA_price'\n\ndaily_corr_DA = []\nfor market in ['ICM', 'rr', 'tertiary', 'rr_down', 'rr_up', 'tertiary_down', 'tertiary_up']:\n    feature2 = f\"{market}_price\"\n    daily_corr_DA_X = prices_norm.groupby(prices_norm.index.floor('D')).apply(lambda x: x[feature1].corr(x[feature2]))\n    daily_corr_DA_X.name = market\n    daily_corr_DA.append(daily_corr_DA_X)\ndaily_corr_DA = pd.concat(daily_corr_DA, axis=1)\ndaily_corr_DA.index = pd.to_datetime(daily_corr_DA.index.date)\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\n\n\n\npx.line(daily_corr_DA)\n\n                            \n                                            \n\n\nFirst of all, something weird is happening after 2025-03-18 between DA and ICM, so we consider data only up to this date.\n\nprices_clean = prices_clean[:'2025-03-18']\n\nprices_daily = prices_clean.groupby(prices_clean.index.date).transform(\"mean\")\nprices_norm = prices_clean - prices_daily\n\nfeature1 = 'DA_price'\ndaily_corr_DA = []\nfor market in ['ICM', 'rr', 'tertiary', 'rr_down', 'rr_up', 'tertiary_down', 'tertiary_up']:\n    feature2 = f\"{market}_price\"\n    daily_corr_DA_X = prices_norm.groupby(prices_norm.index.floor('D')).apply(lambda x: x[feature1].corr(x[feature2]))\n    daily_corr_DA_X.name = market\n    daily_corr_DA.append(daily_corr_DA_X)\ndaily_corr_DA = pd.concat(daily_corr_DA, axis=1)\ndaily_corr_DA.index = pd.to_datetime(daily_corr_DA.index.date)\n\npx.line(daily_corr_DA)\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning:\n\ninvalid value encountered in divide\n\nC:\\Users\\KX6629\\PycharmProjects\\qrm-iberia-sandbox\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning:\n\ninvalid value encountered in divide\n\n\n\n                            \n                                            \n\n\nInterestingly, the daily correlation with DA is very high for ICM, but not so high for the other markets. Plus, there might be some time-related pattern.\nLet’s see if we can train a simple model to predict these correlations, using only the DA prices (and signals built from it) and calendar features. Specifically, we use:\n\nDA price\ndaily percentiles of DA price distribution\ndaily max-min of DA price\ndaily volatility of DA price\nday-of-week (time of day), month (season), holidays\n\n\ndef build_engineered_features(df_hourly):\n\n    # Ensure datetime index\n    df_hourly = df_hourly.copy()\n    df_hourly.index = pd.to_datetime(df_hourly.index)\n    df_hourly = df_hourly.sort_index()\n\n    # Group by date\n    df_hourly['date'] = df_hourly.index.date\n    grouped = df_hourly.groupby('date')['DA_price']\n\n    # Daily aggregations\n    daily_features = pd.DataFrame(index=grouped.groups.keys())\n    daily_features.index = pd.to_datetime(daily_features.index)\n\n    daily_features['mean_price_DA'] = grouped.mean()\n    daily_features['volatility_price_DA'] = grouped.std()\n\n    # Spread: mean of 4 highest hours - mean of 4 lowest hours\n    def spread(x):\n        return np.mean(np.sort(x)[-4:]) - np.mean(np.sort(x)[:4])\n    daily_features['spread_DA'] = grouped.apply(spread)\n\n    # Quantiles\n    for q in [0.1, 0.9]:\n        daily_features[f'q{int(q*100)}'] = grouped.quantile(q)\n\n    # Calendar features\n    daily_features['day_of_week'] = daily_features.index.dayofweek\n    daily_features['is_weekend'] = (daily_features['day_of_week'] &gt;= 5).astype(int)\n\n    # sin-cos encoding for day of week\n    daily_features['dow_sin'] = np.sin(2 * np.pi * daily_features['day_of_week'] / 7)\n    daily_features['dow_cos'] = np.cos(2 * np.pi * daily_features['day_of_week'] / 7)\n\n    # Month / seasonality\n    daily_features['month'] = daily_features.index.month\n    daily_features['month_sin'] = np.sin(2 * np.pi * daily_features['month'] / 12)\n    daily_features['month_cos'] = np.cos(2 * np.pi * daily_features['month'] / 12)\n\n    # is_holiday\n    daily_features['date'] = daily_features.index.date\n    holidays_list = holidays.country_holidays('Spain')\n    daily_features['is_holiday'] = daily_features.apply(lambda x: x['date'] in holidays_list, axis=1).astype(int)\n\n    # Drop intermediate columns\n    daily_features = daily_features.drop(columns=['day_of_week', 'month', 'date'])\n\n    return daily_features\n\n\ndef train_model(df_full, market, shuffle=False):\n\n    X = df_full.drop(columns=['ICM', 'rr', 'tertiary', 'rr_up', 'rr_down', 'tertiary_up', 'tertiary_down'])\n    y = df_full[market]\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, shuffle=shuffle, random_state=999)\n    \n    model = xgb.XGBRegressor(\n        n_estimators=1000,\n        learning_rate=0.1,\n        max_depth=3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        early_stopping_rounds=500\n    )\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n    \n    # print metrics\n\n    y_train_pred = pd.Series(model.predict(X_train), index=y_train.index)\n    y_train_compare = pd.concat((y_train, y_train_pred), axis=1)\n    y_train_compare.columns = ['real', 'pred']\n    rmse_train = root_mean_squared_error(y_train_compare['real'], y_train_compare['pred'])\n    r2_train = r2_score(y_train_compare['real'], y_train_compare['pred'])\n    \n    y_val_pred = pd.Series(model.predict(X_val), index=y_val.index)\n    y_val_compare = pd.concat((y_val, y_val_pred), axis=1)\n    y_val_compare.columns = ['real', 'pred']\n    rmse_val = root_mean_squared_error(y_val_compare['real'], y_val_compare['pred'])\n    r2_val = r2_score(y_val_compare['real'], y_val_compare['pred'])\n\n    print('\\nMetrics')\n    print(f'RMSE = {rmse_val:.2f} (on train {rmse_train:.2f})')\n    print(f'R2 = {r2_val:.2f} (on train {r2_train:.2f})')\n\n    return model, y_train_compare.sort_index(), y_val_compare.sort_index(), X\n\n\ndf_input = build_engineered_features(prices_clean[['DA_price']])\ndf_full = pd.concat((df_input, daily_corr_DA), axis=1).dropna()\n\n\n3.1.1. DA-ICM Correlation\n\nmodel_ICM, y_ICM_train_compare, y_ICM_val_compare, X_ICM = train_model(df_full, market='ICM')\n\n[0] validation_0-rmse:0.10557\n[100]   validation_0-rmse:0.10413\n[200]   validation_0-rmse:0.10488\n[300]   validation_0-rmse:0.10445\n[400]   validation_0-rmse:0.10494\n[500]   validation_0-rmse:0.10522\n[516]   validation_0-rmse:0.10537\n\nMetrics\nRMSE = 0.10 (on train 0.06)\nR2 = 0.05 (on train 0.68)\n\n\n\nbaseline = y_ICM_train_compare['real'].mean()\nprint(f'avg correlation (train) = {baseline:.2f}')\n\nres_train = y_ICM_train_compare['real'] - y_ICM_train_compare['pred']\nres_val = y_ICM_val_compare['real'] - y_ICM_val_compare['pred']\nresiduals = pd.concat((res_train, res_val), axis=1)\nresiduals.columns = ['train', 'val']\npx.scatter(residuals)\n\navg correlation (train) = 0.95\n\n\n                            \n                                            \n\n\n\nexplainer = shap.Explainer(model_ICM)\nshap_values = explainer(X_ICM)\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\n\nThe model has very low explicability compared with a simple average (\\(R^2 \\approx 0\\)): this suggests that the DA-ICM correlation is approximately constant.\nStill, we can extract some insights based on the SHAP values:\n\nLow DA spreads, DA price mean and DA price volatility tend to correspond to lower correlations.\nHigh dow_sin tend to increase correlation, meaning that the correlation is higher at the beginning of the week.\nCorrelation seems to be lower in weekend.\n\nLet’s check if 1. is correct:\n\nfor feature in ['spread_DA', 'mean_price_DA', 'volatility_price_DA']:\n    c = df_full['ICM'].corr(df_full[feature])\n    print(f'correlation with {feature}: {c:.2f}')\n\ncorrelation with spread_DA: 0.50\ncorrelation with mean_price_DA: 0.46\ncorrelation with volatility_price_DA: 0.48\n\n\nConfirmed.\nLet’s check if 2. and 3. are correct:\n\ndf_full.groupby(df_full.index.weekday)['ICM'].mean()\n\n0    0.968008\n1    0.974179\n2    0.968287\n3    0.966126\n4    0.956163\n5    0.944386\n6    0.922590\nName: ICM, dtype: float64\n\n\nIndeed, it appears that the correlation starts high on Monday, peaks on Tuesday, and then it deacreases steadily till its minimum on Sunday. Still, these differences are very small, so it’s hard to take some serious conclusions from them.\nFor completeness – although the SHAP does not suggest anything clear about it – let’s look at how the correlation changes with the month:\n\ndf_full.groupby(df_full.index.month)['ICM'].mean()\n\n1     0.967852\n2     0.962693\n3     0.911377\n4     0.882804\n5     0.953331\n6     0.965981\n7     0.987082\n8     0.987368\n9     0.963260\n10    0.965747\n11    0.973478\n12    0.973522\nName: ICM, dtype: float64\n\n\nThe minimum is in March-April, while the maximum is in July-August.\nfeature: how many hours had DA_price &lt; X?\n\n\n3.1.2 DA-RR Correlation\n\nmodel_RR, y_RR_train_compare, y_RR_val_compare, X_RR = train_model(df_full, market='rr', shuffle=True)\n\n[0] validation_0-rmse:0.37480\n[100]   validation_0-rmse:0.33859\n[200]   validation_0-rmse:0.34803\n[300]   validation_0-rmse:0.35676\n[400]   validation_0-rmse:0.36506\n[500]   validation_0-rmse:0.36961\n[556]   validation_0-rmse:0.37181\n\nMetrics\nRMSE = 0.33 (on train 0.28)\nR2 = 0.24 (on train 0.51)\n\n\n\nbaseline = y_RR_train_compare['real'].mean()\nprint(f'avg correlation (train) = {baseline:.2f}')\n\nres_train = y_RR_train_compare['real'] - y_RR_train_compare['pred']\nres_val = y_RR_val_compare['real'] - y_RR_val_compare['pred']\nresiduals = pd.concat((res_train, res_val), axis=1)\nresiduals.columns = ['train', 'val']\npx.scatter(residuals)\n\navg correlation (train) = 0.20\n\n\n                            \n                                            \n\n\n\nexplainer = shap.Explainer(model_RR)\nshap_values = explainer(X_RR)\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\n\nThe model, has somewhat nontrivial explicability, and the feature importance suggests the following insights:\n\nLow month_cos and month_sin increase correlation: this means that the correlation is highest in June-August, and lowest in January-March.\nLow spread_DA increases correlation.\nA high DA mean and a high p90 in DA price both increase correlation.\nCorrelation tends to be higher in weekends and holidays.\n\nLet’s check if 1. is correct:\n\ndf_full.groupby(df_full.index.month)['rr'].mean()\n\n1     0.058954\n2    -0.008614\n3     0.071115\n4     0.172490\n5     0.270356\n6     0.425666\n7     0.570667\n8     0.449398\n9     0.112262\n10    0.086793\n11    0.134247\n12    0.261808\nName: rr, dtype: float64\n\n\nConfirmed.\nLet’s check if 4. is correct:\n\ndf_full.groupby('is_weekend')['rr'].mean()\n\nis_weekend\n0    0.167997\n1    0.296010\nName: rr, dtype: float64\n\n\n\ndf_full.groupby('is_holiday')['rr'].mean()\n\nis_holiday\n0    0.197146\n1    0.481286\nName: rr, dtype: float64\n\n\nBoth confirmed.\n\n\n3.1.3 DA-Tertiary Correlation\n\nmodel_tertiary, y_tertiary_train_compare, y_tertiary_val_compare, X_tertiary = train_model(df_full, market='tertiary', shuffle=True)\n\n[0] validation_0-rmse:0.12647\n[100]   validation_0-rmse:0.12361\n[200]   validation_0-rmse:0.12660\n[300]   validation_0-rmse:0.13022\n[400]   validation_0-rmse:0.13352\n[500]   validation_0-rmse:0.13527\n[513]   validation_0-rmse:0.13567\n\nMetrics\nRMSE = 0.12 (on train 0.11)\nR2 = 0.17 (on train 0.26)\n\n\n\nbaseline = y_tertiary_train_compare['real'].mean()\nprint(f'avg correlation (train) = {baseline:.2f}')\n\nres_train = y_tertiary_train_compare['real'] - y_tertiary_train_compare['pred']\nres_val = y_tertiary_val_compare['real'] - y_tertiary_val_compare['pred']\nresiduals = pd.concat((res_train, res_val), axis=1)\nresiduals.columns = ['train', 'val']\npx.scatter(residuals)\n\navg correlation (train) = 0.03\n\n\n                            \n                                            \n\n\n\nexplainer = shap.Explainer(model_tertiary)\nshap_values = explainer(X_tertiary)\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\n\nIn this case, the average correlation is 0.03, and our model’s RMSE is 0.12, so the model’s error is 4 times bigger that the mean (and very close to the standard deviation of the correlation). Hence, this model has little to say.\nEven so, we can still take a look at the insights suggested by the SHAP values:\n\nLow DA price mean, volatility, and spread indicate an increased correlation.\nLow month_cos denotes an increased correlation, meaning that there is a higher correlation in the winter months.\nWeekends have a higher correlation.\n\nLet’s test these insights:\n\nfor feature in ['spread_DA', 'mean_price_DA', 'volatility_price_DA']:\n    c = df_full['tertiary'].corr(df_full[feature])\n    print(f'correlation with {feature}: {c:.2f}')\n\ncorrelation with spread_DA: -0.27\ncorrelation with mean_price_DA: -0.24\ncorrelation with volatility_price_DA: -0.24\n\n\nConfirmed 1.\n\ndf_full.groupby(df_full.index.month)['tertiary'].mean()\n\n1     0.006702\n2     0.018153\n3     0.016510\n4     0.064568\n5     0.095479\n6     0.035404\n7     0.033392\n8     0.020599\n9     0.033212\n10    0.007869\n11    0.020392\n12    0.003874\nName: tertiary, dtype: float64\n\n\nAs suggested by 2., we see that the lowest correlation is in the quarter December-March.\n\ndf_full.groupby(df_full.index.weekday)['tertiary'].mean()\n\n0    0.018332\n1    0.018468\n2    0.009772\n3    0.022679\n4    0.016681\n5    0.046788\n6    0.068159\nName: tertiary, dtype: float64\n\n\nConfirmed 3.: Saturday and Sunday have higher average correlation, thought he in-week dispersion is also high."
  },
  {
    "objectID": "posts/BESS-revenues.html#daily-spreads-by-market",
    "href": "posts/BESS-revenues.html#daily-spreads-by-market",
    "title": "1. Get Data",
    "section": "3.2 Daily Spreads by Market",
    "text": "3.2 Daily Spreads by Market\nIn the previous section, we have already computed the market spread for the DA market, and used it as an input for our models. However, this time series (which we can compute for all markets, not just DA) is of interest in its own right, as it likely captures well the available cashflow (at least, if the battery participate in a single market).\nIn the following, we compute and analyze the market spreads for each market, and then attempt to relate the spread in non-DA markets with various indicators built from the DA market (such as the DA market spread itself) as well as calendar features – as done above for the correlations.\nNote that, for the markets when there is double pricing, the spread must be defined properly. Specifically, recall that the “up_price” is the price at which excess energy is sold, while “down_price” is the price at which energy is bought back; hence, given that a battery tries to sell high and buy low, the spread is max(up_price) - min(down_price).\n\nprices_copy = prices.copy()\n\n\ndef spread(x):\n    return np.mean(np.sort(x)[-4:]) - np.mean(np.sort(x)[:4])\n\ndef spread_rr(x):\n    return np.mean(np.sort(x['rr_up_price'])[-4:]) - np.mean(np.sort(x['rr_down_price'])[:4])\n\ndef spread_tertiary(x):\n    return np.mean(np.sort(x['tertiary_up_price'])[-4:]) - np.mean(np.sort(x['tertiary_down_price'])[:4])\n\ndef spread_secondary(x):\n    return np.mean(np.sort(x['secondary_up_price'])[-4:]) - np.mean(np.sort(x['secondary_down_price'])[:4])\n\nspreads = pd.DataFrame()\nspreads['DA_spread'] = prices_copy.groupby(prices_copy.index.date)['DA_price'].apply(spread)\nspreads['ICM_spread'] = prices_copy.groupby(prices_copy.index.date)['ICM_price'].apply(spread)\nspreads['rr_spread'] = prices_copy.groupby(prices_copy.index.date).apply(spread_rr)\nspreads['tertiary_spread'] = prices_copy.groupby(prices_copy.index.date).apply(spread_tertiary)\nspreads.index = pd.to_datetime(spreads.index)\n\n\npx.line(spreads)\n\n                            \n                                            \n\n\nWe see that rr_spread is pretty wild. Any comment on this?\nFirst of all, let’s compute the correlation among them.\n\nspreads.corr()\n\n\n\n\n\n\n\n\nDA_spread\nICM_spread\nrr_spread\ntertiary_spread\n\n\n\n\nDA_spread\n1.000000\n0.82399\n-0.067148\n0.124995\n\n\nICM_spread\n0.823990\n1.00000\n-0.057900\n0.148880\n\n\nrr_spread\n-0.067148\n-0.05790\n1.000000\n0.024118\n\n\ntertiary_spread\n0.124995\n0.14888\n0.024118\n1.000000\n\n\n\n\n\n\n\nLet’s focus on the correlation with DA_spread, and see if it is stable over time – by computing it monthly:\n\nfeature1 = 'DA_spread'\n\nmonthly_corr_DA = []\nfor market in ['ICM', 'rr', 'tertiary']:\n    feature2 = f\"{market}_spread\"\n    monthly_corr_DA_X = spreads.groupby(spreads.index.to_period('M')).apply(lambda x: x[feature1].corr(x[feature2]))\n    monthly_corr_DA_X.name = market\n    monthly_corr_DA.append(monthly_corr_DA_X)\nmonthly_corr_DA = pd.concat(monthly_corr_DA, axis=1)\nmonthly_corr_DA.index = monthly_corr_DA.index.to_timestamp()\n\n\npx.line(monthly_corr_DA)\n\n                            \n                                            \n\n\nThe correlation between DA spread and the spreads of other markets is hardly stable – especially with AASS. This suggests that more complex models are needed, to understand how the spreads of other markets depend on what goes on in the DA market.\n\n# TODO!!!"
  },
  {
    "objectID": "posts/geometric-brownian-motion.html",
    "href": "posts/geometric-brownian-motion.html",
    "title": "Geometric Brownian Motion",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n\nTheory\nA Brownian process (or “random walk”) \\(B_t\\) is such that \\[\n\\nabla B_t \\sim \\mathcal N(0, \\sigma^2)\n\\] If we exponentiate it, we get a geometric Brownian process, \\(S_t\\). In other words, \\(S_t\\) is such that \\[\n\\nabla \\ln(S_t) \\sim \\mathcal N(0, \\sigma^2)\n\\]\n\n\nSynthetic Data and Visualization\n\nran = np.random.RandomState(seed=42)\n\n\nwhite_noise = ran.normal(0, 0.1, 1000)\ndf = pd.DataFrame(white_noise, columns=['white noise'])\n\ndf['brownian process'] = df['white noise'].cumsum()\n\ndf['geometric brownian process'] = np.exp(df['brownian process'])\n\n\npx.line(df)"
  },
  {
    "objectID": "posts/hello.html",
    "href": "posts/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/hello.html#polar-axis",
    "href": "posts/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]